{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MINI-GPT'...\n",
      "remote: Enumerating objects: 77, done.\u001b[K\n",
      "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
      "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
      "remote: Total 77 (delta 41), reused 60 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (77/77), 1.83 MiB | 12.22 MiB/s, done.\n",
      "Resolving deltas: 100% (41/41), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/GboyeStack-Robotics-ML-Engineer/MINI-GPT.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MINI-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd MINI-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-03-16 14:24:56,808\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "\n",
      "View detailed results here: /root/ray_results/TorchTrainer_2025-03-16_14-24-58\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-03-16_14-24-55_284502_1192/artifacts/2025-03-16_14-24-58/TorchTrainer_2025-03-16_14-24-58/driver_artifacts`\n",
      "\n",
      "Training started with configuration:\n",
      "╭────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                            │\n",
      "├────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/BATCH_SIZE                            16 │\n",
      "│ train_loop_config/DECODER_EMBED_DIM                     64 │\n",
      "│ train_loop_config/DECODER_MLP_ratio                      2 │\n",
      "│ train_loop_config/ENCODER_EMBED_DIM                     64 │\n",
      "│ train_loop_config/ENCODER_MLP_ratio                      2 │\n",
      "│ train_loop_config/EPOCHS                                 2 │\n",
      "│ train_loop_config/NUM_DECODER_HEAD                       2 │\n",
      "│ train_loop_config/NUM_DECODER_LAYER                      2 │\n",
      "│ train_loop_config/NUM_ENCODER_HEAD                       2 │\n",
      "│ train_loop_config/NUM_ENCODER_LAYER                      2 │\n",
      "│ train_loop_config/SEED                                   0 │\n",
      "│ train_loop_config/TOKENIZER                                ... special=True),\n",
      "} │\n",
      "│ train_loop_config/TRAIN_PATH          ...PT/DATA/Train.csv │\n",
      "│ train_loop_config/VALID_PATH          ...PT/DATA/Train.csv │\n",
      "│ train_loop_config/VOCAB_SIZE                         32100 │\n",
      "│ train_loop_config/lr                                0.0001 │\n",
      "╰────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TorchTrainer pid=1537)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=1537)\u001b[0m - (node_id=9a9a8cd94692d88cc39aaab1ecfc7eba8121759033e6db6e00df0519, ip=172.19.2.2, pid=1578) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=1537)\u001b[0m - (node_id=9a9a8cd94692d88cc39aaab1ecfc7eba8121759033e6db6e00df0519, ip=172.19.2.2, pid=1577) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=1578)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(RayTrainWorker pid=1578)\u001b[0m /opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "\u001b[36m(RayTrainWorker pid=1578)\u001b[0m   self.pid = os.fork()\n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|              | 0.00/251 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1577) \u001b[0mTraining| Epoch 0/2 0:   0%|              | 0.00/251 [00:00<?, ?it/s]\u001b[0m\u001b[A2025-03-16 14:25:24,942\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_707f0_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=1537, ip=172.19.2.2, actor_id=d8c0e1ae7a6da30b9443807501000000, repr=TorchTrainer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=1577, ip=172.19.2.2, actor_id=c16d172d04bbc11f4c7049bb01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7b9556aea410>)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/kaggle/working/MINI-GPT/TRAIN.py\", line 126, in TrainGpt\n",
      "    outputs = model(**batch_data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: MODEL.forward() got an unexpected keyword argument 'attention_mask'\n",
      "                                                                                \n",
      "\u001b[A                                                                             \n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<00:48, 5.18it/s]\u001b[0m\n",
      "                                                                                48, 5.20it/s]\u001b[0m\u001b[A\n",
      "\u001b[ATraining errored after 0 iterations at 2025-03-16 14:25:24. Total running time: 19s\n",
      "Error file: /tmp/ray/session_2025-03-16_14-24-55_284502_1192/artifacts/2025-03-16_14-24-58/TorchTrainer_2025-03-16_14-24-58/driver_artifacts/TorchTrainer_707f0_00000_0_2025-03-16_14-25-05/error.txt\n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<00:48, 5.16it/s]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1577) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<00:48, 5.18it/s]\u001b[0m\u001b[A2025-03-16 14:25:25,014\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2025-03-16_14-24-58' in 0.0076s.\n",
      "                                                                                \n",
      "\u001b[A                                                                             \n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<00:51, 4.83it/s]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1577) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<00:51, 4.84it/s]\u001b[0m\u001b[A2025-03-16 14:25:25,017\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_707f0_00000]\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=1537, ip=172.19.2.2, actor_id=d8c0e1ae7a6da30b9443807501000000, repr=TorchTrainer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=1577, ip=172.19.2.2, actor_id=c16d172d04bbc11f4c7049bb01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7b9556aea410>)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/kaggle/working/MINI-GPT/TRAIN.py\", line 126, in TrainGpt\n",
      "    outputs = model(**batch_data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: MODEL.forward() got an unexpected keyword argument 'attention_mask'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/MINI-GPT/TRAIN.py\", line 222, in <module>\n",
      "    result = trainer.fit()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 638, in fit\n",
      "    raise TrainingFailedError(\n",
      "ray.train.base_trainer.TrainingFailedError: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\n",
      "To continue this run, you can use: `trainer = TorchTrainer.restore(\"/root/ray_results/TorchTrainer_2025-03-16_14-24-58\")`.\n",
      "To start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.\n",
      "                                                                                \n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=1577)\u001b[0m /opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<03:03, 1.37it/s]\u001b[0m\n",
      "                                                                                02, 1.37it/s]\u001b[0m\u001b[A\n",
      "\u001b[A\u001b[36m(RayTrainWorker pid=1577)\u001b[0m   self.pid = os.fork()                    \n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|       | 1.00/251 [00:00<03:03, 1.36it/s]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1578) \u001b[0mTraining| Epoch 0/2 0:   0%|          | 1.00/251 [00:02<11:39, 2.80s/it]m\u001b[A\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1577) \u001b[0mTraining| Epoch 0/2 0:   0%|          | 1.00/251 [00:02<11:39, 2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "!python TRAIN.py --use_gpu=True --trainer_resources CPU=2 GPU=0 --num_workers=2 --resources_per_worker CPU=1 GPU=1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
